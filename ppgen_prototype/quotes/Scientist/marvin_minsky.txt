American,Scientist
We wanted to solve robot problems and needed some vision, action, reasoning, planning, and so forth. We even used some structural learning, such as was being explored by Patrick Winston.
This is a tricky domain because, unlike simple arithmetic, to solve a calculus problem - and in particular to perform integration - you have to be smart about which integration technique should be used: integration by partial fractions, integration by parts, and so on.
No computer has ever been designed that is ever aware of what it's doing; but most of the time, we aren't either.
Around 1967 Dan Bobrow wrote a program to do algebra problems based on symbols rather than numbers.
There are three basic approaches to AI: Case-based, rule-based, and connectionist reasoning.
The basic idea in case-based, or CBR, is that the program has stored problems and solutions. Then, when a new problem comes up, the program tries to find a similar problem in its database by finding analogous aspects between the problems.
If you just have a single problem to solve, then fine, go ahead and use a neural network. But if you want to do science and understand how to choose architectures, or how to go to a new problem, you have to understand what different architectures can and cannot do.
Societies need rules that make no sense for individuals. For example, it makes no difference whether a single car drives on the left or on the right. But it makes all the difference when there are many cars!
When David Marr at MIT moved into computer vision, he generated a lot of excitement, but he hit up against the problem of knowledge representation; he had no good representations for knowledge in his vision systems.
In general we are least aware of what our minds do best.
I believed in realism, as summarized by John McCarthy's comment to the effect that if we worked really hard, we'd have an intelligent system in from four to four hundred years.
You don't understand anything until you learn it more than one way.
There was a failure to recognize the deep problems in AI; for instance, those captured in Blocks World. The people building physical robots learned nothing.
Stanley Kubrick knew we had good graphics around MIT and came to my lab to find out how to do it. We had some really good stuff. I was very impressed with Kubrick; he knew all the graphics work I had ever heard of, and probably more.
Once when I was standing at the base, they started rotating the set and a big, heavy wrench fell down from the 12 o'clock position of the set, and got buried in the ground a few feet from me. I could have been killed!
Kubrick's vision seemed to be that humans are doomed, whereas Clarke's is that humans are moving on to a better stage of evolution.
I think Lenat is headed in the right direction, but someone needs to include a knowledge base about learning.
I heard that the same thing occurred in a scene in Alien, where the creature pops out of the chest of a crewman. The other actors didn't know what was to happen; the director wanted to get true surprise.
By the way, it was his simulations that helped out in Jurassic Park - without them, there would have been only a few dinosaurs. Based on his techniques, Industrial Light and Magic could make whole herds of dinosaurs race across the screen.
